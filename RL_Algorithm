*Deep-SARSA
    특징: 가치기반, on-policy 방식

    신경망 입/출력: 상태 / 각 행동에 대한 큐함수(근사한 값)

        업데이트 타이밍: 매 타임스탭

    순서:
        1.환경과 상호작용을 통해 S,A(이전 타임스탭에서 정책으로 선택된 행동),R′,S′를 받음
        2.ε-greedy 정책(탐험 or maxQ)을 토대로 A′를 선택
        3.target=Rt​+γQ(S_t+1​,A_t+1​),pred=Q(S_t​,A_t​)을 사용해 오류함수(MSE) L=(target−pred)^2를 계산
        4.역전파 수행
        5.옵티마이저(Adam)로 가중치 업데이트

*Reinforce
    특징: 정책기반(상태에 따라 바로 행동을 선택,정책을 직접 근사), on-policy, G_t를 사용하여 학습

    신경망 입/출력: 상태 / 각 행동을 할 확률(정책)

        출력층: softmax(출력이 정책인 경우는 softmax가 출력의 합이 1이 되게 해주므로 softmax 사용)

        업데이트 타이밍: 매 에피소드

    순서:
        1.에피소드 끝날때까지 (S_1​,A_1​,R_1​),(S_2​,A_2​,R_2​),...,(S_T​,A_T​,R_T​) 수집
        2.G 계산
        3.선택된 행동의 로그확률 계산
        4.오류함수 L(θ)=−logπθ​(A_t​∣S_t​)*G_t​ 계산
        5.오류함수에 대해 역전파를 수행하여 각 파라미터의 그래디언트(∇θ​J(θ)=-E[∇θ​logπθ​(a∣s)*G_t​])를 계산
            원래 E는 기댓값이라 계산 불가능하지만 에피소드 안 모든 타임스탭의 평균으로 근사(몬테카를로 추정)
        6.계산된 그래디언트를 사용하여 옵티마이저(Adam)로 가중치 업데이트
        7.업데이트 된 정책으로 에피소드 반복

*DQN
    특징: 가치기반, off-policy, 경험 리플레이(샘플 간의 상관관계를 없앨 수 있음), 타깃신경망 사용

    온라인신경망 입/출력: 상태(S) / 각 행동에 대한 큐함수

        업데이트 타이밍: 매 타임스탭

    타깃신경망 입/출력: 다음 상태(S′) / 각 행동에 대한 큐함수
        
        타깃신경망 특징: 
            1.타깃신경망의 출력으로 오류함수의 타깃 부분을 계산함 
            2.직접 업데이트 하지 않고 일정주기로 온라인 신경망의 가중치를 복사받음
            3.학습 안정화 역할(타깃값이 매 스텝마다 흔들리는 것을 방지)

        업데이트 타이밍: 사용자 지정(예제 코드 속에서는 매 에피소드)

    순서:
        1.샘플(S,A,R,S′)을 리플레이 메모리에 저장
        2.리플레이 메모리에서 무작위로 미니배치를 추출
        3.타깃 신경망을 사용하여 타깃값 𝑦=𝑅+𝛾max𝑄(𝑆′,A′,𝜃−)을 계산
        4.온라인 신경망의 현재 Q값 𝑄(𝑆,𝐴,𝜃)을 계산
        5.오류함수 L=(y−Q(S,A,θ))^2 의 미니배치 평균을 계산
        6.오류함수에 대해 역전파를 수행하여 온라인 신경망의 그래디언트를 계산
        7.계산된 그래디언트를 사용해 옵티마이저(Adam)로 온라인 신경망의 가중치 업데이트
        8.사용자 지정시점마다 온라인 신경망의 가중치를 타깃 신경망에 복사 

*Actor-critic(A2C)

    특징: 정책기반 + 가치기반 혼합, on-policy, Advantage 사용

        Advantage 함수: A2C 업데이트 식은 θ′←θ+α[∇θ​logπθ​(a∣s)*Q_w​(s,a)]인데 Q함수에 따라 분산이 커져서 
                       가치함수를 베이스라인으로 사용하여 A(s,a)=Q_w(s,a)−V_v(s)​로 나타낼 수 있음
                       하지만 Q와 V를 따로 근사하면 비효율이라 Q함수를 가치함수로 표현하면 
                       δ_v​=R_t​+γV_v(S_t+1​)−V(S_t​)(시간차 에러와 식 동일) 

        reinforce 의 업데이트 식은 θ′←θ+α[∇θ​logπθ​(a∣s)*G_t인데 Q_w를 곱해도 가능한 이유는
        Q_w는 G의 기댓값이라 실제 G값이 아니라 G값의 기댓값을 사용해도 괜찮음 

    액터(정책)신경망 입/출력: 상태 / 각 행동을 할 확률 

        출력층: softmax

        업데이트 타이밍: 크리틱과 같이 업데이트, 보통은 n-step 이후(예제코드는 매 타임스탭)

    크리틱(가치)신경망 입/출력: 상태 / 입력으로 들어오는 상태의 가치함수

        출력층: 선형함수

        업데이트 타이밍: 액터와 같이 업데이트, 보통은 n-step 이후(예제코드는 매 타임스탭)

    순서:
        1.환경과 상호작용을 통해 샘플(S,A,R,S′) 수집 
        2.시간차 에러(δ)를 구하고 어드밴티지 함수를 구함
        3.가치신경망을 MSE 방식으로 L=(정답-예측)^2 = [R_t​+γV_v(S_t+1​)−V(S_t​)]^2 계산 후 역전파 및 옵티마이저(Adam)로 업데이트
        4.정책신경망을 크로스 엔트로피와 어드벤티지 함수로 L​(θ)=−logπθ​(a∣s)⋅A(s,a)를 역전파 수행
        5.옵티마이저(Adam)로 가중치 구한 후 θ′←θ+α[∇θ​logπθ​(a∣s)*A(s,a)] 식으로 업데이트
        
*continuous-actor-critic
    특징:
    신경망 입/출력: